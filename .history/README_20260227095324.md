# GLM-OCR Server

使用 `zai-org/GLM-OCR` 的本地OCR服务器。
通过 FastAPI + 简单的Web UI，可以按页对图像/PDF进行OCR。

![GLM-OCR UI](image.jpg)

## 主要功能

- GLM-OCR推理（`text` / `table` / `formula` / `extract_json`）
- PDF输入（使用 `pypdfium2` 将页面转换为图像后进行OCR）
- 进度显示（`预处理中` -> `正在OCR第i/n页`）
- 执行中断（中断API + UI按钮）
- 换行后处理模式（`none` / `paragraph` / `compact`）
- 页面显示切换（下拉菜单 + `ALL`汇总显示）
- 复制当前显示的结果（`raw` 不在复制范围内）
- 模型/缓存保存在项目目录下

## 运行环境

- Python 3.10+
- Windows 或 Linux/macOS
- 使用CUDA时需要兼容的GPU + 驱动

## 快速开始

### Windows

```bat
run.bat
```

### Linux / macOS

```bash
chmod +x run.sh
./run.sh
```

启动后：

- UI: `http://localhost:8000/`
- API Docs: `http://localhost:8000/docs`

## 使用 `.env` 进行配置

在项目根目录放置 `.env` 文件，`run.sh` / `run.bat` 启动时会读取。

示例：

```env
HOST=0.0.0.0
PORT=9000
TORCH_CHANNEL=cu126
```

主要配置值：

- `HOST`: 绑定地址（默认 `0.0.0.0`）
- `PORT`: 启动端口（默认 `8000`）
- `TORCH_CHANNEL`: PyTorch分发渠道（例如 `cu126`, `cpu`）

## 模型保存位置

模型缓存保存在项目内。

- `models/hf_cache`
- `models/hf_home`

可以通过环境变量 `GLM_MODEL_CACHE` 明确指定。

## API

### `GET /api/status`

返回CUDA可用性、模型ID和缓存目录。

### `POST /api/analyze`

通过多部分表单执行OCR。

表单项目：

- `file` (required): 图像/PDF
- `device`: `auto` / `cuda` / `cpu`
- `dpi`: PDF渲染DPI
- `task`: `text` / `table` / `formula` / `extract_json`
- `linebreak_mode`: `none` / `paragraph` / `compact`
- `schema`: `task=extract_json` 时必需
- `max_new_tokens`
- `temperature`
- `request_id`（可选。未指定时自动编号）

### `GET /api/progress/{request_id}`

获取进度状态。

### `POST /api/cancel/{request_id}`

发送中断请求。
生成过程中会在令牌生成步骤级别进行停止判断。

## 许可证

- 本项目: `LICENSE`（MIT）
- 第三方信息: `THIRD_PARTY_NOTICES.md`